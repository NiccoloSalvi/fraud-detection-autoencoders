@article{Tayebi2025,
  author = {Tayebi, Mohammed and El Kafhali, Said},
  title = {Combining Autoencoders and Deep Learning for Effective Fraud Detection in Credit Card Transactions},
  journal = {Operations Research Forum},
  volume = {6},
  number = {1},
  pages = {8},
  year = {2025},
  publisher = {Springer},
  doi = {10.1007/s43069-024-00409-6},
  url = {https://doi.org/10.1007/s43069-024-00409-6},
  abstract = {The advancement of technologies and the proliferation of new payment services have improved our lives, offering limitless opportunities for individuals and companies in each country to develop their businesses through credit card transactions as a payment method. Consequently, continuous improvement is crucial for these systems, particularly in the classification of fraud transactions. Numerous studies are required in the realm of automated and real-time fraud detection. Due to their advantageous properties, recent studies have utilized different deep learning architectures to create well-fitting models to identify fraudulent transactions. Our proposed solution aims to exploit the robust capabilities of deep learning approaches to identify abnormal transactions. The solution can be presented as follows: To address the imbalanced data set issue, we applied an autoencoder combined with the support vector machine model (ASVM). For the classification phase, we utilize an attention-long short-term memory neural network as a weak learner for the gradient boosting algorithm (GB_ALSTM), comparing it with various techniques, including artificial neural networks (ANNs), convolutional neural networks (CNNs), long short-term memory neural networks (LSTMs), attention-long short-term memory neural networks (ALSTMs), and bidirectional long short-term memory neural networks (BLSTMs). We conducted several experiments on a real-world dataset, revealing promising results in detecting abnormal transactions and highlighting the dominance of our suggested solution over competing models.},
  issn = {2662-2556}
}

@article{Rumelhart1986LearningRB,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536},
  url={https://api.semanticscholar.org/CorpusID:205001834}
}

@article{10.1023/A:1022627411411,
    author = {Cortes, Corinna and Vapnik, Vladimir},
    title = {Support-Vector Networks},
    year = {1995},
    issue_date = {Sept. 1995},
    publisher = {Kluwer Academic Publishers},
    address = {USA},
    volume = {20},
    number = {3},
    issn = {0885-6125},
    url = {https://doi.org/10.1023/A:1022627411411},
    doi = {10.1023/A:1022627411411},
    abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
    journal = {Mach. Learn.},
    month = sep,
    pages = {273–297},
    numpages = {25},
    keywords = {radial basis function classifiers, polynomial classifiers, pattern recognition, neural networks, efficient learning algorithms}
}

@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  institution={Institute for Cognitive Science, University of California, San Diego},
  number={ICS 8504},
  year={1985}
}

@ARTICLE{6795963,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@article{4a848dd1-54e3-3c3c-83c3-04977ded2e71,
 ISSN = {00905364, 21688966},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 urldate = {2025-06-13},
 volume = {29},
 year = {2001}
}
